{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghard(col, mode = 'sequential_fixed', fit = 'fit_transform', params = None,\n",
    "          max_target = 2, max_gap_ratio = 100, log_stretch = 10):\n",
    "    '''\n",
    "    mode : str, default = 'sequential_fixed'\n",
    "        'sequential_fixed' : recursive natural log transformations of quartiles with outliers\n",
    "        'max' : scale outliers to max_target times IQR by compressing the first\n",
    "            and fourth quartiles\n",
    "        'gap_logger': order data, calculate all the gaps between unique values in the data,\n",
    "            get median gap, get the max gap, set the minimum gap to one, and raise gaps\n",
    "            to a polynomial power that scale the max gap to max_gap_ratio times the median gap\n",
    "            (default 100). Rescale data back to 0 to 1\n",
    "        'simple_log': for universally positive integers, takes the natural log.\n",
    "        'dynamic_log': determines if the outliers are at the bottom or top of the range, or both.\n",
    "            For very small/negative outliers shifts the max to -1, scales the min to -log_stretch times\n",
    "                the max-median gap, takes the -log(-x) for each value. Max capped at 0.\n",
    "            For very large outliers shifts the min to 1, scales the max to log_stretch times\n",
    "                the min-median gap, takes the log(x) for each value. Min capped at 0.\n",
    "            For outliers on both ends, shifts the median to 1, scales the max to log_stretch times\n",
    "                the inter-quartile range multiple of the max, takes the log(x) for each value\n",
    "                above the median. shifts the value of the min to log_stretch times\n",
    "                the inter-quartile range multiple of the min + 2, then subtracts 2 from all values\n",
    "                below the median and takes the - log(-x) of thos values (split logging)\n",
    "    fit: string ['fit', 'transform', 'fit_transform'], default = 'fit_transform'\n",
    "        Like with min max scaler, fit returns params, used for consistent transformation.\n",
    "    params: dictionary, default = None\n",
    "        Scaling parameter dictionary generated by fitting.\n",
    "    max_target: float, default = 2\n",
    "        What multiple of the inter quartile range should be the goal of the compression?\n",
    "    max_gap_ratio: int, default = 100.\n",
    "        For gap_logger, what is the ratio of the largest gap to the median.\n",
    "    log_stretch: int, default = 10.\n",
    "        For the dynamic log mode, determines the level of log compression for outliers.\n",
    "    '''\n",
    "\n",
    "    # Perform fit-transform as a sequential call to fit then transform\n",
    "    if fit == 'fit_transform':\n",
    "        params = ghard(col, mode = mode, fit = 'fit', max_target = max_target, max_gap_ratio = max_gap_ratio)\n",
    "        return ghard(col, mode = mode, fit = 'transform', params = params,\n",
    "                     max_target = max_target, max_gap_ratio = max_gap_ratio)\n",
    "\n",
    "    elif fit == 'fit':\n",
    "        # collect parameters to return in a dictionary\n",
    "        params = {}\n",
    "\n",
    "        # confirm data is formatted as a pandas series\n",
    "        if isinstance(col, pd.core.frame.DataFrame):\n",
    "            if col.shape[1] != 1:\n",
    "                raise ValueError('Unsupported datatype: multi-column DataFrame')\n",
    "            col_name = col.columns[0]\n",
    "            col_raw = col[[col_name]].copy().iloc[:, 0]\n",
    "        elif isinstance(col, pd.core.series.Series):\n",
    "            col_raw = col.copy()\n",
    "        elif isinstance(col, np.ndarray):\n",
    "            # TODO: convert all column names to col???\n",
    "            col_raw = pd.Series(col, name = 'col')\n",
    "        else:\n",
    "            raise ValueError('Unsupported datatype: must be a pandas series, dataframe or numpy array')\n",
    "\n",
    "        # ensure column is not single value or binary\n",
    "        if (col_raw.nunique() < 3):\n",
    "            params['transform'] = False\n",
    "            return params\n",
    "\n",
    "        # get interquartile range (IQR)\n",
    "        iq1, iq3 = non_zero_iqr(col_raw, pct = 0.25)\n",
    "\n",
    "        # make sure column has minimum variance\n",
    "        if (iq1 == 0) and (iq3 == 0):\n",
    "            params['transform'] = False\n",
    "            return params\n",
    "\n",
    "        # copy and transform data so IQR is from -1 to 1\n",
    "        col_ = 2 * (col_raw - iq1) / (iq3 - iq1) - 1\n",
    "\n",
    "        # determine the largest outlier\n",
    "        max_ = col_.max()\n",
    "        min_ = col_.min()\n",
    "        max_outlier_abs = max([max_, -min_])\n",
    "\n",
    "        # determine if largest outlier requires compression\n",
    "        if max_outlier_abs < (2 * max_target + 1):\n",
    "            params['transform'] = False\n",
    "            return params\n",
    "\n",
    "        params['mode'] = mode\n",
    "        params['transform'] = True\n",
    "        params['iq1'] = [iq1]\n",
    "        params['iq3'] = [iq3]\n",
    "        params['col_max'] = [max_]\n",
    "        params['col_min'] = [min_]\n",
    "        params['max_outlier_abs'] = max_outlier_abs\n",
    "        params['max_target'] =  max_target\n",
    "\n",
    "        if mode == 'max':\n",
    "            # determine the log base that scales the largest outlier to the\n",
    "            # max_target\n",
    "            log_base = max_outlier_abs**(1/(2 * max_target))\n",
    "            params['log_base'] = log_base\n",
    "\n",
    "            return params\n",
    "\n",
    "        elif mode == 'sequential_fixed':\n",
    "            while (col_.max() > (max_target * 2 + 1)) or (col_.min() < - (max_target * 2 + 1)):\n",
    "                if col_.max() > (max_target * 2 + 1):\n",
    "                    if col_.min() < -np.log(max_target * 2 + 1):\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'both'))\n",
    "                    else:\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'right'))\n",
    "                elif col_.min() < - (max_target * 2 + 1):\n",
    "                    if col_.max() > np.log(max_target * 2 + 1):\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'both'))\n",
    "                    else:\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'left'))\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                # recompute new min and max\n",
    "                max_ = col_.max()\n",
    "                min_ = col_.min()\n",
    "\n",
    "                params['col_max'].append(max_)\n",
    "                params['col_min'].append(min_)\n",
    "\n",
    "\n",
    "            return params\n",
    "\n",
    "        elif mode == 'gap_logger':\n",
    "            # what maw percent of unique values allowed in one mesh section\n",
    "            MAX_FRACTION = .05\n",
    "\n",
    "            # how many unique values needed in the data\n",
    "            MIN_UNIQUE = 3\n",
    "\n",
    "            # if the gap ratio is fixed, how much larger should the largest be\n",
    "            # from the median\n",
    "            max_gap_ratio = 1000\n",
    "\n",
    "            # if the target ratio between the largest gap and median one should\n",
    "            # scale with the size of the current gap\n",
    "            dynamic_gap_ratio = True\n",
    "\n",
    "            if col_raw.nunique() <= MIN_UNIQUE:\n",
    "                params['transform'] = False\n",
    "                return params\n",
    "\n",
    "            # increments that evenly divide 2\n",
    "            mesh_grids = [.0001, .0002, .0004, .0005, .0008, .001, .00125, .0016,\n",
    "                          .002, .0025, .004, .005, .00625, .008, .01, .0125, .02,\n",
    "                          .025, .03125, .04, .05, .0625, .1, .125, .25]\n",
    "\n",
    "            # get original column min, max and name\n",
    "            raw_min = col.min()\n",
    "            raw_max = col.max()\n",
    "            params['raw_max'] = raw_max\n",
    "            params['raw_min'] = raw_min\n",
    "            col_name = str(col_raw.name)\n",
    "\n",
    "            # fit the data to a min -1, max of 1\n",
    "            col_ = 2 * (col_raw - raw_min) / (raw_max - raw_min) - 1\n",
    "\n",
    "            # put unique values in a sorted numpy array and dataframe\n",
    "            unique_vals = np.sort(col_.dropna().unique())\n",
    "            unique_df = pd.DataFrame(unique_vals, columns = [col_name])\n",
    "\n",
    "            # calculate the max number of values allowable in a window\n",
    "            window = int(len(unique_df) * MAX_FRACTION) + 1\n",
    "\n",
    "            # look at spacing of the unique values in df\n",
    "            unique_df['diff'] = unique_df[col_name].diff(window)\n",
    "            min_win = unique_df['diff'].min()\n",
    "\n",
    "            # assign a mesh that has enough detail to capture features, but large\n",
    "            # enough to not overfit the fit column\n",
    "            mesh_grid = max([x for x in mesh_grids if x < min_win] + [0.00005])\n",
    "\n",
    "            # get all the mesh grid intervals, put in a dataframe tag id =1\n",
    "            steps = round(2/mesh_grid)\n",
    "            grid = [x/steps - 1 for x in range(2 * steps + 1)]\n",
    "            grid_df = pd.DataFrame(grid, columns = [col_name])\n",
    "            grid_df['id'] = 1\n",
    "\n",
    "            # get the uniqe values in a df with id = 2\n",
    "            unique_df = unique_df.drop(columns= ['diff'])\n",
    "            unique_df['id'] = 2\n",
    "\n",
    "            # merge mesh and unique real data points, sort together\n",
    "            unique_mesh = pd.concat([unique_df, grid_df], axis  = 0)\n",
    "            unique_mesh = unique_mesh.sort_values(by = [col_name, 'id'], ascending = [1,0])\n",
    "\n",
    "            # make a column of ids from the row above\n",
    "            unique_mesh['shift_id'] = unique_mesh['id'].shift(1).fillna(2)\n",
    "\n",
    "            # compute type. seqential mesh points have type 0 and can be deleted\n",
    "            unique_mesh['type'] = unique_mesh['id'] * unique_mesh['id'] - \\\n",
    "                                  unique_mesh['shift_id']\n",
    "            unique_mesh = unique_mesh.loc[unique_mesh['type'] != 0]\n",
    "\n",
    "            # separate mesh points from actual data for computing the mesh grid\n",
    "            unique_mesh = unique_mesh.sort_values(by = ['id', col_name], ascending = [0,1])\n",
    "\n",
    "            # get the gaps between the unique values\n",
    "            unique_mesh['gaps'] = unique_mesh[col_name].diff()\n",
    "\n",
    "            # set the mesh gaps to NAN so won't affect gap stats\n",
    "            unique_mesh.loc[unique_mesh['id'] == 1, 'gaps'] = np.nan\n",
    "\n",
    "            # find min, max and media gap size\n",
    "            median_gap = unique_mesh['gaps'].median()\n",
    "            min_gap = unique_mesh['gaps'].min()\n",
    "            max_gap = unique_mesh['gaps'].max()\n",
    "\n",
    "            # how big is the largest gap between data points compared to the mean\n",
    "            current_gap_ratio = max_gap/ median_gap\n",
    "\n",
    "            # THIS NEEDS TUNING!!! JUST ARBITRARY BASED ON A FEW COLUMNS\n",
    "            if dynamic_gap_ratio:\n",
    "                max_gap_ratio = 10 * current_gap_ratio ** 0.5\n",
    "\n",
    "            # protect against zero division, get the exponent that scales median\n",
    "            # gap and maximum gap to the desired ratio\n",
    "            if max_gap == median_gap:\n",
    "                power = 1\n",
    "            else:\n",
    "                power = np.log(max_gap_ratio) / (np.log(max_gap) - np.log(median_gap))\n",
    "            params['power'] = power\n",
    "\n",
    "            # normalize the gap values with power scaling\n",
    "            def power_scale_gaps(x):\n",
    "                if np.isnan(x):\n",
    "                    return x\n",
    "                return (x / min_gap)**power\n",
    "\n",
    "            # scale gaps to the defined ratio of max to median\n",
    "            unique_mesh['power_scale_gaps'] = unique_mesh['gaps'].apply(power_scale_gaps)\n",
    "\n",
    "            # fill the NaN created by the shift and cumulatively sum the scaled gaps\n",
    "            unique_mesh['power_scale_gaps_sum'] = unique_mesh['power_scale_gaps'].fillna(0)\n",
    "            unique_mesh['power_scale_gaps_sum'] = unique_mesh['power_scale_gaps_sum'].cumsum()\n",
    "\n",
    "            # bring the mesh points back in sorted order with the column values\n",
    "            unique_mesh = unique_mesh.sort_values(by = [col_name, 'id'], ascending = [1,0])\n",
    "\n",
    "            # due to the sort, every mesh point has a preceeding actual value\n",
    "            # use shift to get the scaled position of that point in the same row\n",
    "            unique_mesh['power_scale_gaps_sum_shift'] = unique_mesh['power_scale_gaps_sum'].shift(1)\n",
    "\n",
    "            # get the gap between the grid point and the preceeding point\n",
    "            unique_mesh[col_name + '_diff'] = unique_mesh[col_name].diff(1)\n",
    "\n",
    "            # copy just the mesh grid points into a new dataframe\n",
    "            # option to clean up with: .drop(columns = drop_cols)\n",
    "            # drop_cols = ['id', 'shift_id', 'type', 'gaps', 'power_scale_gaps', 'power_scale_gaps_sum']\n",
    "            unique_mesh_grid = unique_mesh.loc[unique_mesh['id'] == 1].copy()\n",
    "\n",
    "            # scale the mesh points just like an actual data point using the gap\n",
    "            # from the previous data point\n",
    "            unique_mesh_grid['power_scale_diff'] = unique_mesh_grid[\n",
    "                                    col_name + '_diff'].apply(power_scale_gaps)\n",
    "\n",
    "            # add the scaled position from the actual data to the scaled gap\n",
    "            unique_mesh_grid['final_shift'] = unique_mesh_grid['power_scale_gaps_sum_shift'] + \\\n",
    "                                  unique_mesh_grid['power_scale_diff']\n",
    "\n",
    "            # get the max value of the final dilated dataframe (min will be zero)\n",
    "            max_final = unique_mesh_grid['final_shift'].max()\n",
    "\n",
    "            # divide by range and shift so points are on a -1 to 1 scale\n",
    "            unique_mesh_grid['final_shift_scaled'] = 2 * unique_mesh_grid['final_shift'] / max_final - 1\n",
    "\n",
    "            # save the mesh grid points and their transformed positions\n",
    "            params['grid_shift'] = unique_mesh_grid[[col_name, 'final_shift_scaled']].values\n",
    "\n",
    "            return params\n",
    "\n",
    "        # set up for log transformation\n",
    "        elif mode in ['simple_log', 'dynamic_log']:\n",
    "            # get original column min, max and name\n",
    "            params['raw_max'] = col_raw.max()\n",
    "            params['raw_min'] = col_raw.min()\n",
    "            params['raw_med'] = col_raw.median()\n",
    "\n",
    "            return params\n",
    "\n",
    "        else:\n",
    "            print('Error. No transformation performed')\n",
    "            params['transform'] = False\n",
    "            return params\n",
    "\n",
    "    # transform column\n",
    "    elif fit == 'transform':\n",
    "\n",
    "        # if the fit determined no outliers or improper column\n",
    "        # like boolean, simply return the original column\n",
    "        if (not params['transform']) or params['no_outliers']:\n",
    "            return col\n",
    "\n",
    "        # confirm data is formatted as a pandas series\n",
    "        if isinstance(col, pd.core.frame.DataFrame):\n",
    "            if col.shape[1] != 1:\n",
    "                raise ValueError('Unsupported datatype: multi-column DataFrame')\n",
    "            col_name = col.columns[0]\n",
    "            col_raw = col[[col_name]].copy().iloc[:, 0]\n",
    "        elif isinstance(col, pd.core.series.Series):\n",
    "            col_raw = col.copy()\n",
    "        elif isinstance(col, np.ndarray):\n",
    "            # TODO: convert all column names to col???\n",
    "            col_raw = pd.Series(col, name = 'col')\n",
    "        else:\n",
    "            raise ValueError('Unsupported datatype: must be a pandas series, dataframe or numpy array')\n",
    "\n",
    "        # get interquartile range (IQR)\n",
    "        iq1, iq3 = (params['iq1'][0], params['iq3'][0])\n",
    "\n",
    "        # transform the raw column\n",
    "        col_ = 2 * (col_raw - iq1) / (iq3 - iq1) - 1\n",
    "\n",
    "        # force parameters to match the fit parameters\n",
    "        mode = params['mode']\n",
    "        max_ = params['col_max'][0]\n",
    "        min_ = params['col_min'][0]\n",
    "        max_outlier_abs = params['max_outlier_abs']\n",
    "        max_target = params['max_target']\n",
    "\n",
    "        if mode == 'max':\n",
    "            # determine the log base that scales the largest outlier to the\n",
    "            # max_target\n",
    "            log_base = params['log_base']\n",
    "\n",
    "            col_ = col_.apply(lambda x: log_tail(x, side = 'both', log_base = log_base))\n",
    "\n",
    "            return col_\n",
    "\n",
    "        elif mode == 'sequential_fixed':\n",
    "            count = 1\n",
    "            while (max_ > (max_target * 2 + 1)) or (min_ < - (max_target * 2 + 1)):\n",
    "                if max_ > (max_target * 2 + 1):\n",
    "                    if col_.min() < -np.log(max_target * 2 + 1):\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'both'))\n",
    "                    else:\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'right'))\n",
    "                elif min_ < - (max_target * 2 + 1):\n",
    "                    if max_ > np.log(max_target * 2 + 1):\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'both'))\n",
    "                    else:\n",
    "                        col_ = col_.apply(lambda x: log_tail(x, side = 'left'))\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                # set the min and max from the fit\n",
    "                max_ = params['col_max'][count]\n",
    "                min_ = params['col_min'][count]\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            return col_\n",
    "\n",
    "        elif mode == 'gap_logger':\n",
    "\n",
    "            raw_max = params['raw_max']\n",
    "            raw_min = params['raw_min']\n",
    "            col_name = col_raw.name\n",
    "\n",
    "            # fit the data to a min -1, max of 1\n",
    "            col_ = 2 * (col_raw - raw_min) / (raw_max - raw_min) - 1\n",
    "\n",
    "            # put unique values in a sorted numpy array and dataframe\n",
    "            unique_vals = np.sort(col_.dropna().unique())\n",
    "            unique_df = pd.DataFrame(unique_vals, columns = ['values'])\n",
    "            unique_df['id'] = 2\n",
    "\n",
    "            # get all the mesh grid intervals, put in a dataframe tag id =1\n",
    "            grid = params['grid_shift']\n",
    "            grid_df = pd.DataFrame(grid, columns = ['values', 'transformed'])\n",
    "            grid_df['id'] = 1\n",
    "            grid_df['next_mesh_val'] = grid_df['values'].shift(-1).fillna(1)\n",
    "            grid_df['next_trans_val'] = grid_df['transformed'].shift(-1).fillna(1)\n",
    "\n",
    "            # merge mesh and unique real data points, sort together\n",
    "            unique_mesh = pd.concat([unique_df, grid_df], axis  = 0)\n",
    "            unique_mesh = unique_mesh.sort_values(by = ['values', 'id'],\n",
    "                                    ascending = [1,1]).reset_index(drop = True)\n",
    "\n",
    "            # loop over the data to shift each value to the transformed mesh\n",
    "            iter_ = unique_mesh.copy()\n",
    "            for row in iter_.itertuples():\n",
    "                if row.values < -1:\n",
    "                    # log transform negative outliers\n",
    "                    unique_mesh.loc[row.Index, 'transformed'] = -np.log(-row.values) - 1\n",
    "                elif row.values > 1:\n",
    "                    # log transform positive outliers\n",
    "                    unique_mesh.loc[row.Index, 'transformed'] = np.log(row.values) + 1\n",
    "                else:\n",
    "                    # if this a mesh value, update the moving windows\n",
    "                    if not np.isnan(row.transformed):\n",
    "                        curr_val = row.values\n",
    "                        next_val = row.next_mesh_val\n",
    "                        curr_trans = row.transformed\n",
    "                        next_trans = row.next_trans_val\n",
    "                    else:\n",
    "                        # for the last value (mesh = 1)\n",
    "                        if (next_val - curr_val) == 0:\n",
    "                            unique_mesh.loc[row.Index, 'transformed'] = curr_val\n",
    "\n",
    "                        # take the percent of the mesh window and scale the data\n",
    "                        # to the proportional value in the transformed mesh\n",
    "                        else:\n",
    "                            pct_val_gap = (row.values - curr_val) / (next_val - curr_val)\n",
    "                            trans_loc = pct_val_gap * (next_trans - curr_trans) + curr_trans\n",
    "                            unique_mesh.loc[row.Index, 'transformed'] = trans_loc\n",
    "\n",
    "            # map each value to the new transformed position\n",
    "            transformation = pd.Series(unique_mesh['transformed'].values,\n",
    "                                       index=unique_mesh['values'].values).to_dict()\n",
    "\n",
    "            #transform the column and shift to 0-1 scale\n",
    "            col_trans = col_.map(transformation)\n",
    "            col_trans = (col_trans + 1) / 2\n",
    "\n",
    "            return col_trans\n",
    "\n",
    "        elif mode == 'simple_log':\n",
    "            # get fit columns max and min\n",
    "            raw_max = params['raw_max']\n",
    "            raw_min = params['raw_min']\n",
    "            max_outlier_abs = params['max_outlier_abs']\n",
    "\n",
    "            # fit the data to a min of 1\n",
    "            if raw_min < 1:\n",
    "                shift = 1 - raw_min\n",
    "            else:\n",
    "                shift = 0\n",
    "            col_ = col_raw.values + shift\n",
    "\n",
    "            # protect against infinite logs, cap at 1\n",
    "            col_ = np.where(col_ < 1, 1, col_)\n",
    "\n",
    "            return pd.Series(np.log(col_), name = col_raw.name)\n",
    "\n",
    "        elif mode == 'dynamic_log':\n",
    "            # get fit columns max, min, and median\n",
    "            raw_max = params['raw_max']\n",
    "            raw_min = params['raw_min']\n",
    "            raw_med = params['raw_med']\n",
    "\n",
    "            # determine if median is closer to min or max\n",
    "            med_loc = (raw_med - raw_min) / (raw_max - raw_min)\n",
    "\n",
    "            # TODO: make this a hyperparamter?\n",
    "            median_imbalance = 0.1\n",
    "\n",
    "            if med_loc < median_imbalance: # large outlier only\n",
    "                # stretch the data from 1 to a positive value determined by\n",
    "                # how skewed the data is\n",
    "                scale = 1 / max(med_loc, .00001) * log_stretch\n",
    "                col_ = scale * (col_raw.values - raw_min) / (raw_max - raw_min) + 1\n",
    "\n",
    "                # protect against infinite logs, cap at 1\n",
    "                col_ = np.where(col_ < 1, 1, col_)\n",
    "\n",
    "                return pd.Series(np.log(col_), name = col_raw.name)\n",
    "\n",
    "            elif med_loc > (1 - median_imbalance): # small outlier only\n",
    "                # stretch the data from -1 to a negative value determined by\n",
    "                # how skewed the data is\n",
    "                scale = 1 / max((1 - med_loc), .00001) * log_stretch\n",
    "                col_ = scale * (col_raw.values - raw_min) / (raw_max - raw_min) - scale - 1\n",
    "\n",
    "                # protect against infinite logs, cap at 1\n",
    "                col_ = np.where(col_ > -1, -1, col_)\n",
    "\n",
    "                return pd.Series(-np.log(-col_), name = col_raw.name)\n",
    "\n",
    "            else: # outliers on both extremes\n",
    "                # get how many multiples of the IQR the outliers were\n",
    "                max_iqr = params['col_max'][0]\n",
    "                min_iqr = params['col_min'][0]\n",
    "\n",
    "                # stretch the data proportional to how many iqr intervals each outlier was\n",
    "                # and shift it so median is at 1\n",
    "                scale = (max_iqr - min_iqr) * log_stretch\n",
    "                shift = scale * (raw_med - raw_min) / (raw_max - raw_min)\n",
    "                col_ = scale * (col_raw - raw_min) / (raw_max - raw_min) - shift\n",
    "\n",
    "                # take the log of the data >= 1 and -log(-x) of all other data,\n",
    "                # shifted so all values are < -1\n",
    "                col_ = np.where(col_ < 1, -np.log(-(col_ - 2)), np.log(col_))\n",
    "\n",
    "                return pd.Series(col_, name = col_raw.name)\n",
    "\n",
    "        else:\n",
    "            print('Something went wrong! No transformation performed.')\n",
    "            return col\n",
    "\n",
    "    else:\n",
    "        print('Something went wrong! No transformation performed.')\n",
    "        return None\n",
    "\n",
    "def log_tail(x, side = 'both', log_base = False):\n",
    "    '''log compresses tail distributions after scaling IQR to -1 to 1.\n",
    "    side: ['both', 'left', 'right', 'right_only', 'left_only']\n",
    "        Which tail to compress.\n",
    "    log_base is used to scale the compression to the outlier max value\n",
    "        in 'max' style compression\n",
    "    '''\n",
    "    if not isinstance(x, (int, float,)):\n",
    "        raise ValueError('Non-numerical value passed')\n",
    "\n",
    "    # for sequential natural log\n",
    "    if not log_base:\n",
    "        if (x >= -1) and (x <= 1):\n",
    "            return x\n",
    "        elif x > 1:\n",
    "            if side in ['both', 'right']:\n",
    "                return np.log(x) + 1\n",
    "            else:\n",
    "                return x\n",
    "        elif x < 1:\n",
    "            if side in ['both', 'left']:\n",
    "                return - np.log(-x) - 1\n",
    "            else:\n",
    "                return x\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    # for symmetric max\n",
    "    else:\n",
    "        if (x >= -1) and (x <= 1):\n",
    "            return x\n",
    "        elif x > 1:\n",
    "            return np.log(x) / np.log(log_base) + 1\n",
    "        elif x < 1:\n",
    "            return - np.log(-x) / np.log(log_base) - 1\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "def non_zero_iqr(col, pct = 0.25):\n",
    "    '''Caluclates the inter quartile range (IQR). If zero, expands the search for a\n",
    "    non-zero interval between percentiles symmetric around the median and returns\n",
    "    those values. If the 0.1 percentile and 99.9 percentile values are the same,\n",
    "    returns (0, 0) to indicate that minimal variance is not present in the data.'''\n",
    "    # get IQR, default is 25th and 75th percentile\n",
    "    iq1 = col.quantile([pct]).iloc[0]\n",
    "    iq3 = col.quantile([1 - pct]).iloc[0]\n",
    "\n",
    "    # if IQR is zero, expand range\n",
    "    if iq3 - iq1 == 0:\n",
    "        # TEMP DELETE\n",
    "        # print('iq1 = IQ3')\n",
    "        for i in [x/1000 for x in range(int((1 - pct) * 1000), 1001)]:\n",
    "            iq1 = col.quantile([1 - i]).iloc[0]\n",
    "            iq3 = col.quantile([i]).iloc[0]\n",
    "            # TEMP DELETE\n",
    "            # print(f'i {i} iq1 {iq1} iq3 {iq3}')\n",
    "            if iq3 - iq1 != 0:\n",
    "                # TEMP DELETE\n",
    "                # print('break')\n",
    "                break\n",
    "        # TEMP DELETE\n",
    "        # print(f'OUT OF LOOP i = {i} i == 1 {i == 1}')\n",
    "\n",
    "        # if > 99.9% of values the same\n",
    "        if i == 1:\n",
    "            return (0, 0)\n",
    "\n",
    "    return (iq1, iq3)\n",
    "\n",
    "def non_zero_iqr_percentile(col, pct = 0.25):\n",
    "    '''Caluclates the percentile in the data for the first non-zero\n",
    "    inter quartile range (IQR). If the 0.1 percentile and 99.9 percentile values are the same,\n",
    "    returns 0 to indicate that minimal variance is not present in the data.'''\n",
    "    # get IQR, default is 25th and 75th percentile\n",
    "    iq1 = col.quantile([pct]).iloc[0]\n",
    "    iq3 = col.quantile([1 - pct]).iloc[0]\n",
    "\n",
    "    # if IQR is zero, expand range\n",
    "    if iq3 - iq1 == 0:\n",
    "        for i in [x/1000 for x in range(int((1 - pct) * 1000), 1001)]:\n",
    "            iq1 = col.quantile([1 - i]).iloc[0]\n",
    "            iq3 = col.quantile([i]).iloc[0]\n",
    "            if iq3 - iq1 != 0:\n",
    "                break\n",
    "        # if > 99.9% of values the same\n",
    "        if i == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return i\n",
    "\n",
    "    return pct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
